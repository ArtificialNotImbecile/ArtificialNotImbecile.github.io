{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# https://towardsdatascience.com/how-to-implement-an-adam-optimizer-from-scratch-76e7b217f1cc\n",
    "class AdamOptim():\n",
    "    def __init__(self, eta=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.m_dw, self.v_dw = 0, 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "    def update(self, t, w, dw):\n",
    "        ## dw are from current minibatch\n",
    "        ## momentum beta 1\n",
    "        # *** weights *** #\n",
    "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
    "\n",
    "        ## rms beta 2\n",
    "        # *** weights *** #\n",
    "        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2)\n",
    "\n",
    "        ## bias correction\n",
    "        m_dw_corr = self.m_dw/(1-self.beta1**t)\n",
    "        v_dw_corr = self.v_dw/(1-self.beta2**t)\n",
    "        ## update weights\n",
    "        w = w - self.eta*(m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.projection = torch.nn.Linear(3, 2, bias=False)\n",
    "        torch.nn.init.constant_(self.projection.weight, 0.5)\n",
    "        self.act = torch.nn.SiLU()\n",
    "    def forward(self, x):\n",
    "        z = self.act(self.projection(x))\n",
    "        return z\n",
    "\n",
    "X = torch.randint(0, 10, (10, 3)).float()\n",
    "Y = torch.randint(0, 10, (10, 2)).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch_size=10, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optr = torch.optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23.3192, 15.8141, 17.0207],\n",
      "        [10.1205,  9.3564, 11.1408]])\n",
      "Parameter containing:\n",
      "tensor([[0.2668, 0.3419, 0.3298],\n",
      "        [0.3988, 0.4064, 0.3886]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "opt = optr(net.parameters(), lr=0.01)\n",
    "net.train()\n",
    "opt.zero_grad()\n",
    "logit = net(X[:5])\n",
    "loss = torch.nn.functional.mse_loss(logit, Y[:5])\n",
    "(loss/2).backward()\n",
    "logit = net(X[5:])\n",
    "loss = torch.nn.functional.mse_loss(logit, Y[5:])\n",
    "(loss/2).backward()\n",
    "opt.step()\n",
    "print(net.projection.weight.grad)\n",
    "print(net.projection.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23.3192, 15.8141, 17.0207],\n",
      "        [10.1205,  9.3564, 11.1408]])\n",
      "Parameter containing:\n",
      "tensor([[0.2668, 0.3419, 0.3298],\n",
      "        [0.3988, 0.4064, 0.3886]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "opt = optr(net.parameters(), lr=0.01)\n",
    "net.train()\n",
    "opt.zero_grad()\n",
    "logit = net(X)\n",
    "loss = torch.nn.functional.mse_loss(logit, Y)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "print(net.projection.weight.grad)\n",
    "print(net.projection.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23.3192, 15.8141, 17.0207],\n",
      "        [10.1205,  9.3564, 11.1408]])\n",
      "SGD:\n",
      "tensor([[0.2668, 0.3419, 0.3298],\n",
      "        [0.3988, 0.4064, 0.3886]])\n",
      "Adam:\n",
      "[[0.49 0.49 0.49]\n",
      " [0.49 0.49 0.49]]\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.train()\n",
    "logit = net(X)\n",
    "loss = torch.nn.functional.mse_loss(logit, Y)\n",
    "loss.backward()\n",
    "print(net.projection.weight.grad)\n",
    "print(\"SGD:\")\n",
    "print(net.projection.weight.data - 0.01*net.projection.weight.grad)\n",
    "print(\"Adam:\")\n",
    "print(AdamOptim(eta=0.01).update(1, net.projection.weight.data.numpy(), net.projection.weight.grad.data.numpy())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch_size=1, lr=0.001, DO NOT USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[233.1915, 158.1407, 170.2069],\n",
      "        [101.2046,  93.5642, 111.4084]])\n",
      "Parameter containing:\n",
      "tensor([[0.2668, 0.3419, 0.3298],\n",
      "        [0.3988, 0.4064, 0.3886]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "opt = optr(net.parameters(), lr=0.001) # only for sgd, not for adam. For adam, it's funny that loss/batch vs loss is the same..\n",
    "net.train()\n",
    "opt.zero_grad()\n",
    "for x, y in zip(X, Y):\n",
    "    logit = net(x)\n",
    "    loss = torch.nn.functional.mse_loss(logit, y)\n",
    "    loss.backward()\n",
    "opt.step()\n",
    "print(net.projection.weight.grad)\n",
    "print(net.projection.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch_size=1, lr=0.01, loss/10, USE THIS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23.3191, 15.8141, 17.0207],\n",
      "        [10.1205,  9.3564, 11.1408]])\n",
      "Parameter containing:\n",
      "tensor([[0.2668, 0.3419, 0.3298],\n",
      "        [0.3988, 0.4064, 0.3886]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "net = Net()\n",
    "opt = optr(net.parameters(), lr=0.01)\n",
    "net.train()\n",
    "opt.zero_grad()\n",
    "for x, y in zip(X, Y):\n",
    "    logit = net(x)\n",
    "    loss = torch.nn.functional.mse_loss(logit, y)/10 # this will lead to problematic metric\n",
    "    loss.backward()\n",
    "opt.step()\n",
    "print(net.projection.weight._grad)\n",
    "print(net.projection.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
